{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**\n",
    "- Despite having “regression” in its name, a logistic regression is actually a widely used binary classifier.\n",
    "$P(y_i=1 \\mid X)={\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}x)}}}$.\n",
    "<br>Where: \n",
    "- $P(y_i=1 \\mid X)$ is the probability of the i-th observation’s target value. \n",
    "- $y_i$ is being class 1 \n",
    "- X is the training data\n",
    "- $\\beta_{0}$ and $\\beta_{1}$ are the parameters to be learned\n",
    "- e is Euler’s number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression object\n",
    "clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# Train model\n",
    "model = clf.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation\n",
    "new_observation = [[.4, .4, .4, .4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.12005191, 0.3633989 , 0.51654918]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View predicted probabilities\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Logistic Regression Using SAG solver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression object using sag solver\n",
    "clf_sag = LogisticRegression(random_state=0, solver='sag')\n",
    "\n",
    "# Train model\n",
    "model_sag = clf_sag.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[0.06133167, 0.51990398, 0.41876435]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sag.predict(new_observation), model_sag.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Cross-Validation To Find The Best Value Of C** - Fast C Hyperparameter Tuning.\n",
    "<br>Sometimes the characteristics of a learning algorithm allows us to search for the best hyperparameters significantly faster than either brute force or randomized model search methods. scikit-learn’s LogisticRegressionCV method includes a parameter Cs. If supplied a list, Cs is the candidate hyperparameter values to select from. If supplied a integer, Cs a list of that many candidate values will is drawn from a logarithmic scale between 0.0001 and and 10000 (a range of reasonable values for C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=100, class_weight=None, cv='warn', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='warn', n_jobs=None,\n",
       "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create cross-validated logistic regression\n",
    "clf_linear = linear_model.LogisticRegressionCV(Cs=100)\n",
    "\n",
    "# Train model\n",
    "clf_linear.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 Regularization** (also called least absolute deviations)\n",
    "<br>is show the effect of the regularization parameter C on the coefficients and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remake the variable, keeping all data where the category is not 2.\n",
    "X_not2 = X[y != 2]\n",
    "y_not2 = y[y != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_not2[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_not2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_not2_train, X_not2_test, y_not2_train, y_not2_test = train_test_split(X_not2, y_not2, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the regularization penalty is comprised of the sum of the absolute value of the coefficients, we need to scale the data so the coefficients are all based on the same scale.\n",
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_not2_train_std = sc.fit_transform(X_not2_train)\n",
    "\n",
    "# Apply the scaler to the test data\n",
    "X_not2_test_std = sc.transform(X_not2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 10\n",
      "Coefficient of each feature:  [[-0.09553031 -3.71983564  4.39258717  0.        ]]\n",
      "Training accuracy:  0.9857142857142858\n",
      "Test accuracy:  1.0\n",
      "C: 1\n",
      "Coefficient of each feature:  [[ 0.         -2.27460208  2.56783887  0.        ]]\n",
      "Training accuracy:  0.9857142857142858\n",
      "Test accuracy:  1.0\n",
      "C: 0.1\n",
      "Coefficient of each feature:  [[ 0.         -0.8214314   0.97187017  0.        ]]\n",
      "Training accuracy:  0.9857142857142858\n",
      "Test accuracy:  1.0\n",
      "C: 0.005\n",
      "Coefficient of each feature:  [[0. 0. 0. 0.]]\n",
      "Training accuracy:  0.5\n",
      "Test accuracy:  0.5\n",
      "C: 0.001\n",
      "Coefficient of each feature:  [[0. 0. 0. 0.]]\n",
      "Training accuracy:  0.5\n",
      "Test accuracy:  0.5\n",
      "C: 0.0001\n",
      "Coefficient of each feature:  [[0. 0. 0. 0.]]\n",
      "Training accuracy:  0.5\n",
      "Test accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "C = [10, 1, 0.1, 0.005, 0.001, 0.0001]\n",
    "\n",
    "for c in C:\n",
    "    clf_l1 = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n",
    "    clf_l1.fit(X_not2_train, y_not2_train)\n",
    "    print('C:', c)\n",
    "    print('Coefficient of each feature: ', clf_l1.coef_)\n",
    "    print('Training accuracy: ', clf_l1.score(X_not2_train_std, y_not2_train))\n",
    "    print('Test accuracy: ', clf_l1.score(X_not2_test_std, y_not2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Vs. Rest Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-vs-rest logistic regression object\n",
    "clf_ovr = LogisticRegression(random_state=0, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model_ovr = clf_ovr.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), array([[0.12005191, 0.3633989 , 0.51654918]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ovr.predict(new_observation), model_ovr.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling Imbalanced Classes In Logistic Regression**\n",
    "<br>Like many other learning algorithms in scikit-learn, LogisticRegression comes with a built-in method of handling imbalanced classes. If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the class_weight parameter to weight the classes to make certain we have a balanced mix of each class. Specifically, the balanced argument will automatically weigh classes inversely proportional to their frequency:\n",
    "$w_j = \\frac{n}{kn_{j}}$\n",
    "where:\n",
    "- $w_j$ is the weight to class \n",
    "- j, n is the number of observations, \n",
    "- $n_j$ is the number of observations in class j\n",
    "- k is the total number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create decision tree classifer object\n",
    "clf_balanced = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "\n",
    "# Train model\n",
    "model_balanced = clf_balanced.fit(X_not2_train_std, y_not2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[0.24801442, 0.75198558]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_balanced.predict(new_observation), model_balanced.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expansion about solver parameter**\n",
    "[Source](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hypothesis h(x), takes an input and gives us the estimated output value.\n",
    "<br>This hypothesis can be a as simple as a one variable linear equation,... up to a very complicated and long multivariate equation with respect to the type of the algorithm we’re using (i.e. linear regression, logistic regression..etc).\n",
    "<br><img src=\"https://i.stack.imgur.com/i8OO5.png\"/><br>\n",
    "- Our task is to find the best Parameters (a.k.a Thetas or Weights) that give us the least error in predicting the output. We call this error a Cost or Loss Function and apparently our goal is to minimize it in order to get the best predicted output!\n",
    "- One more thing to recall, that the relation between the parameter value and its effect on the cost function (i.e. the error) looks like a bell curve (i.e. Quadratic; recall this because it’s very important) .\n",
    "- So if we start at any point in that curve and if we keep taking the derivative (i.e. tangent line) of each point we stop at, we will end up at what so called the Global Optima as shown in this image:\n",
    "<br><img src=\"https://i.stack.imgur.com/mZ9UU.png\"/><br>\n",
    "If we take the partial derivative at minimum cost point (i.e. global optima) we find the slope of the tangent line = 0 (then we know that we reached our target).\n",
    "<br>\n",
    "That’s valid only if we have Convex Cost Function, but if we don’t, we may end up stuck at what so called Local Optima; consider this non-convex function:\n",
    "<br><img src=\"https://i.stack.imgur.com/WYEux.png\"/><br>\n",
    "Now you should have the intuition about the hack relationship between what we are doing and the terms: Deravative, Tangent Line, Cost Function, Hypothesis ..etc.\n",
    "<br>\n",
    "*Side Note: The above mentioned intuition also related to the Gradient Descent Algorithm (see later)*\n",
    "\n",
    "**Background**\n",
    "\n",
    "Linear Approximation:\n",
    "<br>\n",
    "Given a function, $f(x)$, we can find its tangent at $x=a$. The equation of the tangent line $L(x)$ is: $L(x)=f(a)+f′(a)(x−a)$.\n",
    "Take a look at the following graph of a function and its tangent line:\n",
    "<br><img src=\"https://i.stack.imgur.com/u0vU0.png\"/><br>\n",
    "From this graph we can see that near $x=a$, the tangent line and the function have nearly the same graph. On occasion we will use the tangent line, $L(x)$, as an approximation to the function, $f(x)$, near $x=a$. In these cases we call the tangent line the linear approximation to the function at $x=a$.\n",
    "\n",
    "**Quadratic Approximation:**\n",
    "<br>\n",
    "Same like linear approximation but this time we are dealing with a curve but we cannot find the point near to 0 by using the tangent line.\n",
    "<br>\n",
    "Instead, we use a parabola (which is a curve where any point is at an equal distance from a fixed point or a fixed straight line), like this:\n",
    "<br><img src=\"https://i.stack.imgur.com/Yd2mE.png\"/><br>\n",
    "And in order to fit a good parabola, both parabola and quadratic function should have same value, same first derivative, AND second derivative, ...the formula will be (just out of curiosity): $Qa(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)(x-a)^2}{2}$\n",
    "<br>\n",
    "Now we should be ready to do the comparison in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Newton’s Method**\n",
    "\n",
    "Recall the motivation for gradient descent step at x: we minimize the quadratic function (i.e. Cost Function).\n",
    "\n",
    "Newton’s method uses in a sense a better quadratic function minimisation. A better because it uses the quadratic approximation (i.e. first AND second partial derivatives).\n",
    "\n",
    "You can imagine it as a twisted Gradient Descent with The Hessian (The Hessian is a square matrix of second-order partial derivatives of order nxn).\n",
    "\n",
    "Moreover, the geometric interpretation of Newton's method is that at each iteration one approximates  f(x) by a quadratic function around xn, and then takes a step towards the maximum/minimum of that quadratic function (in higher dimensions, this may also be a saddle point). Note that if f(x) happens to be a quadratic function, then the exact extremum is found in one step.\n",
    "\n",
    "*Drawbacks:*\n",
    "\n",
    "1. It’s computationally expensive because of The Hessian Matrix (i.e. second partial derivatives calculations).\n",
    "\n",
    "2. It attracts to Saddle Points which are common in multivariable optimization (i.e. a point its partial derivatives disagree over whether this input should be a maximum or a minimum point!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm:**\n",
    "\n",
    "In a nutshell, it is analogue of the Newton’s Method but here the Hessian matrix is approximated using updates specified by gradient evaluations (or approximate gradient evaluations). In other words, using an estimation to the inverse Hessian matrix.\n",
    "\n",
    "The term Limited-memory simply means it stores only a few vectors that represent the approximation implicitly.\n",
    "\n",
    "If I dare say that when dataset is small, L-BFGS relatively performs the best compared to other methods especially it saves a lot of memory, however there are some “serious” drawbacks such that if it is unsafeguarded, it may not converge to anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A Library for Large Linear Classification:**\n",
    "\n",
    "It’s a linear classification that supports logistic regression and linear support vector machines (A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics i.e feature value).\n",
    "\n",
    "The solver uses a coordinate descent (CD) algorithm that solves optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes.\n",
    "\n",
    "*LIBLINEAR* is the winner of ICML 2008 large-scale learning challenge. It applies Automatic parameter selection (a.k.a L1 Regularization) and it’s recommended when you have high dimension dataset (recommended for solving large-scale classification problems)\n",
    "\n",
    "*Drawbacks:*\n",
    "\n",
    "It may get stuck at a non-stationary point (i.e. non-optima) if the level curves of a function are not smooth.\n",
    "\n",
    "Also cannot run in parallel.\n",
    "\n",
    "It cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary classifiers are trained for all classes.\n",
    "\n",
    "*Side note: According to Scikit Documentation: The “**liblinear**” solver is used by **default** for historical reasons.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Stochastic Average Gradient: - SAG**\n",
    "\n",
    "SAG method optimizes the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods.\n",
    "\n",
    "It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\n",
    "\n",
    "*Drawbacks:*\n",
    "\n",
    "It only supports **L2 penalization**.\n",
    "Its memory cost of O(N), which can make it impractical for large N (because it remembers the most recently computed values for approx. all gradients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. SAGA:**\n",
    "\n",
    "The SAGA solver is a variant of SAG that also supports the non-smooth penalty=l1 option (i.e. L1 Regularization). This is therefore the solver of choice for sparse multinomial logistic regression and it’s also suitable very Large dataset.\n",
    "\n",
    "*Side note: According to Scikit Documentation: The **SAGA solver** is often the best choice.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "<br><img src=\"https://i.stack.imgur.com/K568D.png\"/><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
