{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exam3-thang","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPXyOo1+4SCj88J/tsYiqvN"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7rqR6xsfBryP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1595128364352,"user_tz":-540,"elapsed":3138,"user":{"displayName":"Duc Thang Luu","photoUrl":"https://lh6.googleusercontent.com/-9pv_MaomS7I/AAAAAAAAAAI/AAAAAAAAAf4/nNDEccmr_7w/s64/photo.jpg","userId":"11920885927812896586"}},"outputId":"0e8c6d1e-530c-42f4-f258-300bda83b6ed"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n","    -O ./rps-test-set.zip"],"execution_count":10,"outputs":[{"output_type":"stream","text":["--2020-07-19 03:12:42--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 108.177.125.128, 74.125.23.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 29516758 (28M) [application/zip]\n","Saving to: ‘./rps-test-set.zip’\n","\n","\r./rps-test-set.zip    0%[                    ]       0  --.-KB/s               \r./rps-test-set.zip  100%[===================>]  28.15M  --.-KB/s    in 0.1s    \n","\n","2020-07-19 03:12:42 (208 MB/s) - ‘./rps-test-set.zip’ saved [29516758/29516758]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UTcd62oHD04D","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArMizJuMxukc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595132815548,"user_tz":-540,"elapsed":4267865,"user":{"displayName":"Duc Thang Luu","photoUrl":"https://lh6.googleusercontent.com/-9pv_MaomS7I/AAAAAAAAAAI/AAAAAAAAAf4/nNDEccmr_7w/s64/photo.jpg","userId":"11920885927812896586"}},"outputId":"242757ea-7b0a-4243-f3cd-54c8c6bbf14d"},"source":["# ======================================================================\n","# There are 5 questions in this exam with increasing difficulty from 1-5.\n","# Please note that the weight of the grade for the question is relative\n","# to its difficulty. So your Category 1 question will score significantly\n","# less than your Category 5 question.\n","#\n","# Don't use lambda layers in your model.\n","# You do not need them to solve the question.\n","# Lambda layers are not supported by the grading infrastructure.\n","#\n","# You must use the Submit and Test button to submit your model\n","# at least once in this category before you finally submit your exam,\n","# otherwise you will score zero for this category.\n","# ======================================================================\n","#\n","# Computer Vision with CNNs\n","#\n","# Build a classifier for Rock-Paper-Scissors based on the rock_paper_scissors\n","# TensorFlow dataset.\n","#\n","# IMPORTANT: Your final layer should be as shown. Do not change the\n","# provided code, or the tests may fail\n","#\n","# IMPORTANT: Images will be tested as 150x150 with 3 bytes of color depth\n","# So ensure that your input layer is designed accordingly, or the tests\n","# may fail. \n","#\n","# NOTE THAT THIS IS UNLABELLED DATA. \n","# You can use the ImageDataGenerator to automatically label it\n","# and we have provided some starter code.\n","\n","\n","import urllib.request\n","import zipfile\n","import tensorflow as tf\n","from keras_preprocessing.image import ImageDataGenerator\n","\n","def solution_model():\n","    url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n","    urllib.request.urlretrieve(url, 'rps.zip')\n","    local_zip = 'rps.zip'\n","    zip_ref = zipfile.ZipFile(local_zip, 'r')\n","    zip_ref.extractall('./')\n","    zip_ref.close()\n","\n","    local_zip = './rps-test-set.zip'\n","    zip_ref = zipfile.ZipFile(local_zip, 'r')\n","    zip_ref.extractall('./')\n","    zip_ref.close()\n","\n","    TRAINING_DIR = \"./rps/\"\n","    training_datagen = ImageDataGenerator(\n","        rescale = 1./255,\n","        rotation_range=40,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest')\n","\n","    train_generator = training_datagen.flow_from_directory(\n","        TRAINING_DIR,\n","        target_size=(150,150),\n","        class_mode='categorical'\n","    )\n","\n","\n","    VALIDATION_DIR = \"./rps-test-set/\"\n","    validation_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","    validation_generator = validation_datagen.flow_from_directory(\n","        VALIDATION_DIR,\n","        target_size=(150,150),\n","        class_mode='categorical'\n","    )\n","\n","    model = tf.keras.models.Sequential([\n","        # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n","        # This is the first convolution\n","        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n","        tf.keras.layers.MaxPooling2D(2, 2),\n","        # The second convolution\n","        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        # The third convolution\n","        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        # The fourth convolution\n","        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        # Flatten the results to feed into a DNN\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dropout(0.5),\n","        # 512 neuron hidden layer\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dense(3, activation='softmax')\n","    ])\n","\n","    model.summary()\n","    model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","    history = model.fit(train_generator, epochs=25, validation_data = validation_generator, verbose = 1)\n","\n","    return model\n","\n","\n","# Note that you'll need to save your model as a .h5 like this.\n","# When you press the Submit and Test button, your saved .h5 model will\n","# be sent to the testing infrastructure for scoring\n","# and the score will be returned to you.\n","if __name__ == '__main__':\n","    model = solution_model()\n","    model.save(\"mymodel.h5\")\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Found 2520 images belonging to 3 classes.\n","Found 372 images belonging to 3 classes.\n","Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_28 (Conv2D)           (None, 148, 148, 64)      1792      \n","_________________________________________________________________\n","max_pooling2d_28 (MaxPooling (None, 74, 74, 64)        0         \n","_________________________________________________________________\n","conv2d_29 (Conv2D)           (None, 72, 72, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_29 (MaxPooling (None, 36, 36, 64)        0         \n","_________________________________________________________________\n","conv2d_30 (Conv2D)           (None, 34, 34, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_30 (MaxPooling (None, 17, 17, 128)       0         \n","_________________________________________________________________\n","conv2d_31 (Conv2D)           (None, 15, 15, 128)       147584    \n","_________________________________________________________________\n","max_pooling2d_31 (MaxPooling (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 512)               3211776   \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 3)                 1539      \n","=================================================================\n","Total params: 3,473,475\n","Trainable params: 3,473,475\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/25\n","79/79 [==============================] - 169s 2s/step - loss: 1.5137 - accuracy: 0.3437 - val_loss: 1.0893 - val_accuracy: 0.3360\n","Epoch 2/25\n","79/79 [==============================] - 170s 2s/step - loss: 0.9696 - accuracy: 0.5369 - val_loss: 0.5334 - val_accuracy: 0.6801\n","Epoch 3/25\n","79/79 [==============================] - 168s 2s/step - loss: 0.6154 - accuracy: 0.7329 - val_loss: 0.2885 - val_accuracy: 0.9651\n","Epoch 4/25\n","79/79 [==============================] - 168s 2s/step - loss: 0.3924 - accuracy: 0.8448 - val_loss: 0.1418 - val_accuracy: 0.9516\n","Epoch 5/25\n","79/79 [==============================] - 170s 2s/step - loss: 0.2822 - accuracy: 0.8905 - val_loss: 0.0696 - val_accuracy: 0.9919\n","Epoch 6/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.2485 - accuracy: 0.9135 - val_loss: 0.1232 - val_accuracy: 0.9516\n","Epoch 7/25\n","79/79 [==============================] - 168s 2s/step - loss: 0.1915 - accuracy: 0.9325 - val_loss: 0.0257 - val_accuracy: 1.0000\n","Epoch 8/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1752 - accuracy: 0.9389 - val_loss: 0.0226 - val_accuracy: 1.0000\n","Epoch 9/25\n","79/79 [==============================] - 169s 2s/step - loss: 0.1390 - accuracy: 0.9512 - val_loss: 0.0373 - val_accuracy: 1.0000\n","Epoch 10/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1478 - accuracy: 0.9512 - val_loss: 0.1118 - val_accuracy: 0.9570\n","Epoch 11/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1223 - accuracy: 0.9595 - val_loss: 0.0950 - val_accuracy: 0.9543\n","Epoch 12/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1245 - accuracy: 0.9575 - val_loss: 0.0607 - val_accuracy: 0.9704\n","Epoch 13/25\n","79/79 [==============================] - 170s 2s/step - loss: 0.1073 - accuracy: 0.9631 - val_loss: 0.1589 - val_accuracy: 0.9059\n","Epoch 14/25\n","79/79 [==============================] - 168s 2s/step - loss: 0.1110 - accuracy: 0.9639 - val_loss: 0.0703 - val_accuracy: 0.9651\n","Epoch 15/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1094 - accuracy: 0.9639 - val_loss: 0.1407 - val_accuracy: 0.9597\n","Epoch 16/25\n","79/79 [==============================] - 170s 2s/step - loss: 0.0996 - accuracy: 0.9718 - val_loss: 0.0427 - val_accuracy: 0.9758\n","Epoch 17/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1005 - accuracy: 0.9651 - val_loss: 0.0150 - val_accuracy: 0.9946\n","Epoch 18/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.0681 - accuracy: 0.9782 - val_loss: 0.0840 - val_accuracy: 0.9624\n","Epoch 19/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.0594 - accuracy: 0.9813 - val_loss: 0.1112 - val_accuracy: 0.9651\n","Epoch 20/25\n","79/79 [==============================] - 170s 2s/step - loss: 0.0952 - accuracy: 0.9694 - val_loss: 0.1209 - val_accuracy: 0.9570\n","Epoch 21/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.1074 - accuracy: 0.9667 - val_loss: 0.0631 - val_accuracy: 0.9624\n","Epoch 22/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.0741 - accuracy: 0.9778 - val_loss: 0.0825 - val_accuracy: 0.9597\n","Epoch 23/25\n","79/79 [==============================] - 170s 2s/step - loss: 0.0671 - accuracy: 0.9794 - val_loss: 0.0419 - val_accuracy: 0.9973\n","Epoch 24/25\n","79/79 [==============================] - 167s 2s/step - loss: 0.0885 - accuracy: 0.9746 - val_loss: 0.0988 - val_accuracy: 0.9651\n","Epoch 25/25\n","79/79 [==============================] - 166s 2s/step - loss: 0.0786 - accuracy: 0.9798 - val_loss: 0.0292 - val_accuracy: 0.9973\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZGCQrf4HEgj3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}