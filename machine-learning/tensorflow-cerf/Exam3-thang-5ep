{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exam3-thang-5ep","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOmgZHGlEYYsUBBAtEvNvcw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"7rqR6xsfBryP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1595131522109,"user_tz":-540,"elapsed":4194,"user":{"displayName":"Duc Thang Luu","photoUrl":"https://lh6.googleusercontent.com/-9pv_MaomS7I/AAAAAAAAAAI/AAAAAAAAAf4/nNDEccmr_7w/s64/photo.jpg","userId":"11920885927812896586"}},"outputId":"cb1917b1-5698-4db4-a6d9-d4083bb74598"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n","    -O ./rps-test-set.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-07-19 04:05:19--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 64.233.189.128, 108.177.97.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 29516758 (28M) [application/zip]\n","Saving to: ‘./rps-test-set.zip’\n","\n","\r./rps-test-set.zip    0%[                    ]       0  --.-KB/s               \r./rps-test-set.zip  100%[===================>]  28.15M   174MB/s    in 0.2s    \n","\n","2020-07-19 04:05:20 (174 MB/s) - ‘./rps-test-set.zip’ saved [29516758/29516758]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UTcd62oHD04D","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArMizJuMxukc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":785},"executionInfo":{"status":"ok","timestamp":1595132394313,"user_tz":-540,"elapsed":873940,"user":{"displayName":"Duc Thang Luu","photoUrl":"https://lh6.googleusercontent.com/-9pv_MaomS7I/AAAAAAAAAAI/AAAAAAAAAf4/nNDEccmr_7w/s64/photo.jpg","userId":"11920885927812896586"}},"outputId":"81071858-c98a-41db-ab9b-c9bdb43514f9"},"source":["# ======================================================================\n","# There are 5 questions in this exam with increasing difficulty from 1-5.\n","# Please note that the weight of the grade for the question is relative\n","# to its difficulty. So your Category 1 question will score significantly\n","# less than your Category 5 question.\n","#\n","# Don't use lambda layers in your model.\n","# You do not need them to solve the question.\n","# Lambda layers are not supported by the grading infrastructure.\n","#\n","# You must use the Submit and Test button to submit your model\n","# at least once in this category before you finally submit your exam,\n","# otherwise you will score zero for this category.\n","# ======================================================================\n","#\n","# Computer Vision with CNNs\n","#\n","# Build a classifier for Rock-Paper-Scissors based on the rock_paper_scissors\n","# TensorFlow dataset.\n","#\n","# IMPORTANT: Your final layer should be as shown. Do not change the\n","# provided code, or the tests may fail\n","#\n","# IMPORTANT: Images will be tested as 150x150 with 3 bytes of color depth\n","# So ensure that your input layer is designed accordingly, or the tests\n","# may fail. \n","#\n","# NOTE THAT THIS IS UNLABELLED DATA. \n","# You can use the ImageDataGenerator to automatically label it\n","# and we have provided some starter code.\n","\n","\n","import urllib.request\n","import zipfile\n","import tensorflow as tf\n","from keras_preprocessing.image import ImageDataGenerator\n","\n","def solution_model():\n","    url = 'https://storage.googleapis.com/download.tensorflow.org/data/rps.zip'\n","    urllib.request.urlretrieve(url, 'rps.zip')\n","    local_zip = 'rps.zip'\n","    zip_ref = zipfile.ZipFile(local_zip, 'r')\n","    zip_ref.extractall('./')\n","    zip_ref.close()\n","\n","    local_zip = './rps-test-set.zip'\n","    zip_ref = zipfile.ZipFile(local_zip, 'r')\n","    zip_ref.extractall('./')\n","    zip_ref.close()\n","\n","    TRAINING_DIR = \"./rps/\"\n","    training_datagen = ImageDataGenerator(\n","        rescale = 1./255,\n","        rotation_range=40,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True,\n","        fill_mode='nearest')\n","\n","    train_generator = training_datagen.flow_from_directory(\n","        TRAINING_DIR,\n","        target_size=(150,150),\n","        class_mode='categorical'\n","    )\n","\n","\n","    VALIDATION_DIR = \"./rps-test-set/\"\n","    validation_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","    validation_generator = validation_datagen.flow_from_directory(\n","        VALIDATION_DIR,\n","        target_size=(150,150),\n","        class_mode='categorical'\n","    )\n","\n","    model = tf.keras.models.Sequential([\n","        # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n","        # This is the first convolution\n","        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n","        tf.keras.layers.MaxPooling2D(2, 2),\n","        # The second convolution\n","        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        # The third convolution\n","        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        # The fourth convolution\n","        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        # Flatten the results to feed into a DNN\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dropout(0.5),\n","        # 512 neuron hidden layer\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dense(3, activation='softmax')\n","    ])\n","\n","    model.summary()\n","    model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n","    history = model.fit(train_generator, epochs=5, validation_data = validation_generator, verbose = 1)\n","\n","    return model\n","\n","\n","# Note that you'll need to save your model as a .h5 like this.\n","# When you press the Submit and Test button, your saved .h5 model will\n","# be sent to the testing infrastructure for scoring\n","# and the score will be returned to you.\n","if __name__ == '__main__':\n","    model = solution_model()\n","    model.save(\"mymodel.h5\")\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Found 2520 images belonging to 3 classes.\n","Found 372 images belonging to 3 classes.\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 148, 148, 64)      1792      \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 72, 72, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 6272)              0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 6272)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               3211776   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3)                 1539      \n","=================================================================\n","Total params: 3,473,475\n","Trainable params: 3,473,475\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","79/79 [==============================] - 171s 2s/step - loss: 1.3876 - accuracy: 0.3782 - val_loss: 1.0559 - val_accuracy: 0.4785\n","Epoch 2/5\n","79/79 [==============================] - 170s 2s/step - loss: 0.9000 - accuracy: 0.5762 - val_loss: 0.4431 - val_accuracy: 0.9677\n","Epoch 3/5\n","79/79 [==============================] - 172s 2s/step - loss: 0.6198 - accuracy: 0.7353 - val_loss: 0.2701 - val_accuracy: 0.8602\n","Epoch 4/5\n","79/79 [==============================] - 170s 2s/step - loss: 0.4004 - accuracy: 0.8341 - val_loss: 0.1166 - val_accuracy: 0.9946\n","Epoch 5/5\n","79/79 [==============================] - 169s 2s/step - loss: 0.2701 - accuracy: 0.9028 - val_loss: 0.0463 - val_accuracy: 0.9919\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZGCQrf4HEgj3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}