{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar2VPyHWBRBG"
      },
      "source": [
        "# 第5章: PyTorch を用いたトランスクリプトームデータの分類\n",
        "\n",
        "- 澤田 高志\n",
        "- 清水 秀幸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zjmV7lGURdJ"
      },
      "source": [
        "##### 入力5-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh4RhAUXa97a"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWn02N8b5ivi"
      },
      "source": [
        "##### 入力5-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHj0R_tB47qK"
      },
      "outputs": [],
      "source": [
        "# パッケージのインポート\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJcf2he_5oV0"
      },
      "source": [
        "##### 入力5-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qndo1TDo47nM"
      },
      "outputs": [],
      "source": [
        "# バージョンの確認\n",
        "print('numpy: ', np.__version__)\n",
        "print('pandas: ', pd.__version__)\n",
        "print('matplotlib: ', matplotlib.__version__)\n",
        "print('sklearn: ', sklearn.__version__)\n",
        "print('torch: ', torch.__version__)\n",
        "print('seaborn: ', sns.__version__)\n",
        "print('optuna: ', optuna.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb8fadS_jl2l"
      },
      "source": [
        "##### 入力5-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O68nEyuxOFjU"
      },
      "outputs": [],
      "source": [
        "# GPUが使用可能かどうか確認する\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_asOQZ2bOFjU"
      },
      "source": [
        "##### 入力5-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-z0VRkRjl2l"
      },
      "outputs": [],
      "source": [
        "# Numpyのアレイ\n",
        "import numpy as np\n",
        "\n",
        "data = [[1, 2], [3, 4]]\n",
        "np_data = np.array(data)\n",
        "print(np_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPEQQ52S55Zl"
      },
      "source": [
        "##### 入力5-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq-GozV8qQ98"
      },
      "outputs": [],
      "source": [
        "# PyTorchのテンソル\n",
        "import torch\n",
        "\n",
        "tensor_data = torch.tensor(data)\n",
        "print(tensor_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGesfm8oqQ-B"
      },
      "source": [
        "##### 入力5-7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4TZUb3bqQ-B"
      },
      "outputs": [],
      "source": [
        "tensor_from_np = torch.from_numpy(np_data)\n",
        "tensor_from_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6qmEU3QqQ-B"
      },
      "source": [
        "##### 入力5-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbjGxWbWqQ-B"
      },
      "outputs": [],
      "source": [
        "# 前のデータ型のように (like) ランダムなテンソルを作る\n",
        "tensor_random = torch.rand_like(tensor_data, dtype=torch.float)\n",
        "print(tensor_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_Z5Ot1jOFjV"
      },
      "source": [
        "##### 入力5-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu8TLsKRqQ-D"
      },
      "outputs": [],
      "source": [
        "# 4*4の行列の成分がすべて1であるテンソル\n",
        "tensor = torch.ones(4, 4)\n",
        "\n",
        "# GPU上へtensorを動かす\n",
        "tensor.to(device)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EPfKhwR6O48"
      },
      "source": [
        "##### 入力5-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62C6ZM7YOFjV"
      },
      "outputs": [],
      "source": [
        "# Numpyと似たような指定ができる\n",
        "# 値の代入\n",
        "tensor[:, 1] = 0\n",
        "tensor[1, :] = 3\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yby72VmN6Ttk"
      },
      "source": [
        "##### 入力5-11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxyxn3vDOFjV"
      },
      "outputs": [],
      "source": [
        "# 行や列の抽出\n",
        "print('0行目: ', tensor[0])\n",
        "print('1列目: ', tensor[:, 1])\n",
        "print('最終列: ', tensor[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w4jk9HHqQ-D"
      },
      "source": [
        "##### 入力5-12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XpOCjzGqQ-E"
      },
      "outputs": [],
      "source": [
        "# dim=1で横に並べる\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nojp73tRqQ-E"
      },
      "source": [
        "##### 入力5-13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du7q3RPdqQ-E"
      },
      "outputs": [],
      "source": [
        "# テンソルの計算: ここでは行列式\n",
        "tensor @ tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdtOJFLQ6flu"
      },
      "source": [
        "##### 入力5-14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTz2DOZVqQ-E"
      },
      "outputs": [],
      "source": [
        "# アダマール積\n",
        "tensor * tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_SuoAqrqQ-E"
      },
      "source": [
        "##### 入力5-15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FWCrFjDqQ-E"
      },
      "outputs": [],
      "source": [
        "sum_item = tensor.sum().item()\n",
        "print(sum_item, type(sum_item))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGwn7UaRqQ-F"
      },
      "source": [
        "##### 入力5-16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad3cIVkyqQ-F"
      },
      "outputs": [],
      "source": [
        "print(tensor, '\\n')\n",
        "# 'add'ではなく'add_'\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb0cD8amqQ-F"
      },
      "source": [
        "##### 入力5-17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9roHxLBNqQ-F"
      },
      "outputs": [],
      "source": [
        "t = torch.ones(5)\n",
        "print('tensor: ', t)\n",
        "n = t.numpy()\n",
        "print('numpy: ', n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMcn2NDzqQ-F"
      },
      "source": [
        "##### 入力5-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqzKVPBlqQ-F"
      },
      "outputs": [],
      "source": [
        "t.add_(1)\n",
        "print('tensor: ', t)\n",
        "print('numpy: ', n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLUSSgoIqQ-H"
      },
      "source": [
        "##### 入力5-19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7BjXgyYqQ-H"
      },
      "outputs": [],
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmvbX4T361GM"
      },
      "source": [
        "##### 入力5-20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFsPTxPfqQ-H"
      },
      "outputs": [],
      "source": [
        "np.add(n, 3, out=n)\n",
        "print('tensor: ', t)\n",
        "print('numpy: ', n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv_NAKrNqQ-I"
      },
      "source": [
        "##### 入力5-21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKw0bZOcqQ-I"
      },
      "outputs": [],
      "source": [
        "predicted_probability_p = [0.8, 0.2, 0.6]\n",
        "true_label_t = [1.0, 0, 0] # 1.0でなく1にするとテンソルにする際にエラーが起きる\n",
        "\n",
        "# Numpyで計算をしてみよう\n",
        "import numpy as np\n",
        "\n",
        "BCE_0 = -true_label_t[0] * np.log(predicted_probability_p[0])\n",
        "BCE_1 = -(1 - true_label_t[1]) * np.log(1 - predicted_probability_p[1])\n",
        "BCE_2 = -(1 - true_label_t[2]) * np.log(1 - predicted_probability_p[2])\n",
        "print(BCE_0, BCE_1, BCE_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQAoe2FXqQ-I"
      },
      "source": [
        "##### 入力5-22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuNY0lJkqQ-I"
      },
      "outputs": [],
      "source": [
        "np.mean([BCE_0, BCE_1, BCE_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU-vo4BLqQ-I"
      },
      "source": [
        "##### 入力5-23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBpGhgz8qQ-I"
      },
      "outputs": [],
      "source": [
        "# torchでは，BCELoss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "# Numpyのアレイをtorch.tensorでPyTorchで扱える形に変換する\n",
        "p_tensor = torch.tensor(predicted_probability_p)\n",
        "t_tensor = torch.tensor(true_label_t) # 1.0でなく1にするとエラーが起きる\n",
        "loss(p_tensor, t_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZbNREou4UH4"
      },
      "source": [
        "##### 入力5-24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TKS_fTB_ilL"
      },
      "outputs": [],
      "source": [
        "# シグモイド関数を作図\n",
        "# シグモイド関数\n",
        "def sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "# シグモイド関数の導関数\n",
        "def sigmoid_prime(x):\n",
        "      return (np.exp(x)) / ((1 + np.exp(x)) ** 2)\n",
        "\n",
        "# tanh関数を作図\n",
        "# tanh関数\n",
        "def tanh(x):\n",
        "      return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "# tanh関数の導関数\n",
        "def tanh_prime(x):\n",
        "      return 4 / (np.exp(x) + np.exp(-x)) ** 2\n",
        "\n",
        "# 定義域を決める\n",
        "x = np.linspace(-6, 6, 256)\n",
        "# サイズを定める\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 1*2に分けて1番目に描画\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.set_title('Sigmoid and Sigmoid_prime', size=16)\n",
        "ax1.set_xlabel('x', size=16)\n",
        "ax1.set_ylabel('y', size=16)\n",
        "ax1.grid(axis='both')\n",
        "ax1.set_ylim(-1.1,1.1)\n",
        "\n",
        "# sigmoidのグラフを描く\n",
        "ax1.plot(x, sigmoid(x), color='#1f77b4', label='sigmoid')\n",
        "ax1.plot(x, sigmoid_prime(x), color='#ff7f0e', label='sigmoid_prime')\n",
        "ax1.legend()\n",
        "\n",
        "# 1*2に分けて2番目に描画\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.set_title('tanh and tanh_prime', size=16)\n",
        "ax2.set_xlabel('x', size=16)\n",
        "#ax2.set_ylabel('y', size=16)\n",
        "ax2.grid(axis='both')\n",
        "ax2.set_ylim(-1.1,1.1)\n",
        "\n",
        "# tanhのグラフを描く\n",
        "ax2.plot(x, tanh(x), color='#1f77b4', label='tanh')\n",
        "ax2.plot(x, tanh_prime(x), color='#ff7f0e', label='tanh_prime')\n",
        "ax2.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edAPLAW9qQ-J"
      },
      "source": [
        "##### 入力5-25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aazrWAWHED_L"
      },
      "outputs": [],
      "source": [
        "# ReLU関数を作図\n",
        "# ReLU関数\n",
        "def ReLU(x):\n",
        "      return np.maximum(0, x)\n",
        "\n",
        "# ReLU関数の導関数\n",
        "def ReLU_prime(x):\n",
        "      return np.where(x > 0, 1, 0)\n",
        "\n",
        "# 定義域を決める\n",
        "x = np.linspace(-6, 6, 256)\n",
        "# サイズを定める\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "\n",
        "# 1*1に分けて1番目に描画\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_title('ReLU and ReLU_prime', size=16)\n",
        "ax.set_xlabel('x', size=16)\n",
        "ax.set_ylabel('y', size=16)\n",
        "ax.grid(axis='both')\n",
        "\n",
        "# ReLUのグラフを描く\n",
        "ax.plot(x, ReLU(x), color='#1f77b4', label='ReLU')\n",
        "ax.plot(x, ReLU_prime(x), color='#ff7f0e', label='ReLU_prime')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qeTl1E6qQ-K"
      },
      "source": [
        "##### 入力5-26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3KkZr4-qQ-K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "\n",
        "x = np.linspace(-6, 6, 256)\n",
        "y = np.linspace(-6, 6, 256)\n",
        "# メッシュ作成\n",
        "X, Y = np.meshgrid(x, y)\n",
        "# 計算式\n",
        "Loss = X**2 - Y**2\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# x軸が見えやすい方向から見た鞍点\n",
        "# add_subplot(121)とは，1*2に図を並べる際の1番目を指定している\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax1.set_title('Saddle_point', size=16)\n",
        "ax1.set_xlabel('$w_{1}$', size=16)\n",
        "ax1.set_ylabel('$w_{2}$', size=16)\n",
        "ax1.set_zlabel('Loss', size=16)\n",
        "# 鞍点の例: 双曲放物面を描く\n",
        "ax1.plot_wireframe(X, Y, Loss, color='#1f77b4')\n",
        "# 角度を指定\n",
        "ax1.view_init(elev=20, azim=65)\n",
        "# 鞍点はx=y=Loss=0の点である.サイズを大きく設定したが見えにくい\n",
        "ax1.scatter(0, 0, 0, s=300, color='#ff7f0e')\n",
        "\n",
        "# y軸が見えやすい方向から見た鞍点\n",
        "ax2 = fig.add_subplot(122, projection='3d')\n",
        "ax2.set_title('Saddle_point', size=16)\n",
        "ax2.set_xlabel('$w_{1}$', size=16)\n",
        "ax2.set_ylabel('$w_{2}$', size=16)\n",
        "ax2.set_zlabel('Loss', size=16)\n",
        "# 鞍点の例: 双曲放物面を描く\n",
        "ax2.plot_wireframe(X, Y, Loss, color='#1f77b4')\n",
        "# 別の角度の指定\n",
        "ax2.view_init(elev=20, azim=25)\n",
        "# 鞍点はx=y=Loss=0の点である. サイズを大きく設定したが見えにくい\n",
        "ax2.scatter(0, 0, 0, s=300, color='#ff7f0e')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGrOCbarqQ-K"
      },
      "source": [
        "##### 入力5-27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuRhuneE-5C6"
      },
      "outputs": [],
      "source": [
        "# アップロードしたcsvファイルを読み込む\n",
        "gse_mRNA_exprs_normal_selected = pd.read_csv(\n",
        "       '/content/GSE36376_normal.csv', index_col=0\n",
        ")\n",
        "gse_mRNA_exprs_tumor_selected = pd.read_csv(\n",
        "       '/content/GSE36376_tumor.csv', index_col=0\n",
        ")\n",
        "\n",
        "# class labelを作り，正常を0，がんを1とする\n",
        "gse_mRNA_exprs_normal_selected.loc['class'] = 0\n",
        "gse_mRNA_exprs_tumor_selected.loc['class'] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4hvb8ZIMO2T"
      },
      "source": [
        "##### 入力5-28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLlJver8MCEd"
      },
      "outputs": [],
      "source": [
        "# 分類器を作るために，データを分ける.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 転置したものをtrain_test_splitで分ける\n",
        "# まずはtrainvalデータセットとtestデータに分ける\n",
        "# testは全体の25%，trainvalが75%\n",
        "gse_mRNA_exprs_normal_selected_trainval, gse_mRNA_exprs_normal_selected_test = train_test_split(\n",
        "    gse_mRNA_exprs_normal_selected.T, train_size=0.75, random_state=0)\n",
        "gse_mRNA_exprs_tumor_selected_trainval, gse_mRNA_exprs_tumor_selected_test = train_test_split(\n",
        "    gse_mRNA_exprs_tumor_selected.T, train_size=0.75, random_state=0)\n",
        "\n",
        "# 次にtrainvalデータセットをtrainとvalに分ける\n",
        "# trainが全体の50%，valが全体の25%\n",
        "gse_mRNA_exprs_normal_selected_train, gse_mRNA_exprs_normal_selected_val = train_test_split(\n",
        "    gse_mRNA_exprs_normal_selected_trainval, train_size=0.667, random_state=0)\n",
        "gse_mRNA_exprs_tumor_selected_train, gse_mRNA_exprs_tumor_selected_val = train_test_split(\n",
        "    gse_mRNA_exprs_tumor_selected_trainval, train_size=0.667, random_state=0)\n",
        "\n",
        "# trainデータ，validationデータ，testデータを作り出す\n",
        "gse_mRNA_exprs_train = pd.concat(\n",
        "    [gse_mRNA_exprs_normal_selected_train, gse_mRNA_exprs_tumor_selected_train]\n",
        ")\n",
        "X_train = gse_mRNA_exprs_train.iloc[:, 0:-1]\n",
        "y_train = gse_mRNA_exprs_train.iloc[:, -1]\n",
        "\n",
        "gse_mRNA_exprs_val = pd.concat(\n",
        "    [gse_mRNA_exprs_normal_selected_val, gse_mRNA_exprs_tumor_selected_val]\n",
        ")\n",
        "\n",
        "X_val = gse_mRNA_exprs_val.iloc[:, 0:-1]\n",
        "y_val = gse_mRNA_exprs_val.iloc[:, -1]\n",
        "\n",
        "gse_mRNA_exprs_test = pd.concat(\n",
        "    [gse_mRNA_exprs_normal_selected_test, gse_mRNA_exprs_tumor_selected_test]\n",
        ")\n",
        "X_test = gse_mRNA_exprs_test.iloc[:, 0:-1]\n",
        "y_test = gse_mRNA_exprs_test.iloc[:, -1]\n",
        "\n",
        "# 標準化する\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "# StandardScalerのfitメソッドを取ってくる→トレーニングデータからデータ全体の平均値と標準偏差を推定\n",
        "sc.fit(X_train)\n",
        "\n",
        "# トレーニングデータの平均と標準偏差の計算\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_val_std = sc.transform(X_val)\n",
        "X_test_std = sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52ZePaXKFIa"
      },
      "source": [
        "##### 入力5-29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGxUtrcdJuM2"
      },
      "outputs": [],
      "source": [
        "# コードに再現性を持たせるために乱数を固定\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.use_deterministic_algorithms = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMBbzvNiqQ-L"
      },
      "source": [
        "##### 入力5-30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlb_qlu9qQ-L"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 持っているデータをPyTorchが扱えるデータ型に「翻訳」する\n",
        "class TrainValData(Dataset):\n",
        "    # データセットの初期設定 (init)\n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "    # 指定されたindexに対応するサンプルを取ってくる (getitem)\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "    # データセットのサンプル数 (len)\n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "# 誤ってテストデータを訓練に使えないように，\n",
        "# yのラベル情報はないものとしている\n",
        "class TestData(Dataset):\n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "# 訓練データ, 検証データ, テストデータをPyTorchで分割する\n",
        "train_data = TrainValData(torch.FloatTensor(X_train_std), torch.FloatTensor(y_train))\n",
        "val_data = TrainValData(torch.FloatTensor(X_val_std), torch.FloatTensor(y_val))\n",
        "test_data = TestData(torch.FloatTensor(X_test_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDLjAf3kqQ-L"
      },
      "source": [
        "##### 入力5-31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdVQd6cRqQ-L"
      },
      "outputs": [],
      "source": [
        "# データローダーを初期化\n",
        "BATCH_SIZE = 64\n",
        "# 訓練データセットについて，訓練データはサンプル数は216なので，そのままだと1エポック4イテレーション\n",
        "# しかし，最後のイテレーションは24個分のデータしかない. これではノイズの影響が大きいため，\n",
        "# drop_last=Trueにしておく\n",
        "train_loader = DataLoader(\n",
        "      dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGpxQ_3AghTN"
      },
      "source": [
        "##### 入力5-32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3fp9GNjqQ-M"
      },
      "outputs": [],
      "source": [
        "# ニューラルネットワークアーキテクチャの定義\n",
        "import torch.nn as nn\n",
        "\n",
        "num_layers = 3\n",
        "num_features = [32, 128, 64]\n",
        "\n",
        "# モデルの定義のためにクラスを作る\n",
        "class BinaryClassificationNet(nn.Module):\n",
        "    # __init__()関数で使用するレイヤーを定義\n",
        "    def __init__(self):\n",
        "\n",
        "        # 基底クラスとしてnn.moduleを継承する\n",
        "        # ニューラルネットワークの基本機能が使えるようになるおまじないと考えてもよい\n",
        "        super(BinaryClassificationNet, self).__init__()\n",
        "\n",
        "        # 一方通行のモデルを作る際はnn.Sequentialを使うと\n",
        "        # コードがスッキリまとめられる\n",
        "        # 第1層: 入力特徴量は13，出力はnum_features[0]=32\n",
        "        self.layers1 = nn.Sequential(\n",
        "            # nn.Linearは重みとバイアスだけを持つ結合層\n",
        "            nn.Linear(in_features=13, out_features=num_features[0]),\n",
        "            # バッチ正規化\n",
        "            nn.BatchNorm1d(num_features[0]),\n",
        "            # 活性化関数はReLU\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # 第2層: 入力特徴量はnum_features[0]=32，出力はnum_features[1]=128\n",
        "        self.layers2 = nn.Sequential(\n",
        "            nn.Linear(in_features=num_features[0], out_features=num_features[1]),\n",
        "            nn.BatchNorm1d(num_features[1]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        # 第3層: 入力特徴量はnum_features[1]=128，出力はnum_features[2]=64\n",
        "        self.layers3 = nn.Sequential(\n",
        "            nn.Linear(in_features=num_features[1], out_features=num_features[2]),\n",
        "            nn.BatchNorm1d(num_features[2]),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # 最後に先の出力のnum_features[i]を1にする.これは0である確率1つだけということを言う\n",
        "        # 後述するが損失関数でnn.BCEWithLogitsLoss()を使う場合，活性化関数のシグモイドは使わない\n",
        "        self.layers_out = nn.Linear(in_features=num_features[2], out_features=1)\n",
        "    \n",
        "    # 関数forward()では定義した層を積み重ねる\n",
        "    # 図で言えば左から右の，順伝播処理を書いている\n",
        "    def forward(self, x):\n",
        "        x = self.layers1(x)\n",
        "        x = self.layers2(x)\n",
        "        x = self.layers3(x)\n",
        "        x = self.layers_out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xZ8NJXJghTO"
      },
      "source": [
        "##### 入力5-33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpJQQWSqghTO"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# GPUをモデルで使えるようにする\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = BinaryClassificationNet()\n",
        "model.to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# 損失関数にはバイナリー交差エントロピーを示すnn.BCEWithLogitsLoss()を使う\n",
        "# この損失関数では，出力特徴量を1の全結合層(nn.Linear(out_features=1))を受け取る\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "# 重み更新にはAdamを用いる. 汎用性が高く，最も用いられているオプティマイザの1つである\n",
        "LEARNING_RATE = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 学習過程でうまく学習できているか監視するために，損失関数だけでなく\n",
        "# accuracyも見ることにする.そのためにもaccuracyを確認する関数を定義する\n",
        "def binary_acc(y_true, y_pred):\n",
        "    # torch.eq()で2つのテンソルで値が一致する数をカウントする\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RC24aolghTO"
      },
      "source": [
        "##### 入力5-34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voLd5C1NghTO"
      },
      "outputs": [],
      "source": [
        "# エポック数は400にする\n",
        "EPOCHS = 400\n",
        "model.train()\n",
        "# トレーニングと検証を同時に行うforループを作る\n",
        "# まだテストデータセットは使わない\n",
        "\n",
        "# 損失関数の出力とaccuracyはリストにまとめて後で図示する\n",
        "train_loss_li = []\n",
        "train_acc_li = []\n",
        "val_loss_li = []\n",
        "val_acc_li = []\n",
        "\n",
        "# エポック: すべての入力データを何回使えたかを示す\n",
        "for epoch in range(EPOCHS):\n",
        "    # イテレーション: 重みを何度更新したかを示す.訓練データセットは216種，\n",
        "    # バッチサイズ64で，'drop_last=True'より，1エポックあたり3イテレーションである\n",
        "\n",
        "    # 訓練では，まず，ミニバッチごとに分けられたデータをfor文で1つずつ扱う\n",
        "    for X_batch_train, y_batch_train in train_loader:\n",
        "        # 5.1.1項で行ったようにミニバッチのデータ(tensor)をGPU上に動かしている\n",
        "        X_batch_train, y_batch_train = X_batch_train.to(device), y_batch_train.to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "        # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "        # squeezeでいらない次元を減らす\n",
        "        y_logits = model(X_batch_train).squeeze()\n",
        "        # model()はBinaryClassificationNetのモデルだが，その出力は全結合層のものである\n",
        "        # ゆえに，y_logitsはそのままでは確率ではない\n",
        "        # y_logitsをシグモイド関数で予測確率に変換し，その数値を丸めて予測ラベルにする\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "        # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する. ここではaccも使う\n",
        "        loss = loss_fn(y_logits, y_batch_train)\n",
        "        acc = binary_acc(y_true=y_batch_train, y_pred=y_pred)\n",
        "\n",
        "        # 3. オプティマイザの勾配を0に初期化し，過去の勾配の影響を排除して現在の勾配を計算できるようにする\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. 誤差逆伝播法で損失関数の勾配を計算する\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. オプティマイザで重みなどのパラメータ更新を行う\n",
        "        optimizer.step()\n",
        "    \n",
        "    # 訓練データセットでミニバッチ学習が終わったところで，検証データセットの結果も確認する\n",
        "    model.eval()\n",
        "\n",
        "    # 検証では勾配が変動してはならないため，torch.no_grad()でテンソルの勾配計算ができないようにする\n",
        "    with torch.no_grad():\n",
        "        for X_batch_val, y_batch_val in val_loader:\n",
        "            X_batch_val, y_batch_val = X_batch_val.to(device), y_batch_val.to(device)\n",
        "            # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "            val_logits = model(X_batch_val).squeeze()\n",
        "            val_pred = torch.round(torch.sigmoid(val_logits))\n",
        "            # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する. ここではaccも使う\n",
        "            val_loss = loss_fn(val_logits, y_batch_val)\n",
        "            val_acc = binary_acc(y_true=y_batch_val, y_pred=val_pred)\n",
        "            # 3以降は勾配の計算に関わってくるため，検証やテストでは使わない\n",
        "        \n",
        "        # 20エポックごとの結果をまとめる\n",
        "        if epoch % 20 == 0:\n",
        "            print(f'Epoch: {epoch} | Train_loss: {loss:.5f}, Train_acc: {acc:.2f}%')\n",
        "            print(f'Epoch: {epoch} | Val_loss: {val_loss:.5f}, Val_acc: {val_acc:.2f}%')\n",
        "        # 最後に，train_loss，train_acc，val_loss，val_accをリストにまとめる\n",
        "        train_loss_li.append(loss.item())\n",
        "        train_acc_li.append(acc)\n",
        "        val_loss_li.append(val_loss.item())\n",
        "        val_acc_li.append(val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJesn1cmghTO"
      },
      "source": [
        "##### 入力5-35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNZsYuPPghTP"
      },
      "outputs": [],
      "source": [
        "# 全400エポックにおけるtrain_loss，val_loss，train_acc，val_accを確認する\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "# add_subplot(121)とは，1*2に図を並べる際の1番目を指定している\n",
        "ax1 = fig.add_subplot(121)\n",
        "# ax1.set_xlabel('epochs', size = 16)\n",
        "ax1.set_ylabel('loss', size=16)\n",
        "ax1.plot(train_loss_li, label='train_loss')\n",
        "ax1.plot(val_loss_li, color='#ff7f0e', label='val_loss')\n",
        "ax1.axvline(x=100, color='#2ca02c')\n",
        "ax1.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.set_ylabel('acc', size=16)\n",
        "# ax2.set_xlabel('epochs', size = 16)\n",
        "ax2.plot(train_acc_li, label='train_acc')\n",
        "ax2.plot(val_acc_li, color='#ff7f0e', label='val_acc')\n",
        "ax2.axvline(x=100, color='#2ca02c')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPKXfxC1ghTP"
      },
      "source": [
        "##### 入力5-36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiNTUlF7ghTP"
      },
      "outputs": [],
      "source": [
        "# 先人が過去に実装したearly_stoppingのための関数\n",
        "# 日本語部分は著者による説明である\n",
        "# 参考: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    '''Early stopはval_lossが規定回数を経ても改善しなかった場合にそこで終了させる関数である'''\n",
        "\n",
        "    def __init__(\n",
        "            self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print\n",
        "    ):\n",
        "\n",
        "        '''\n",
        "        Args:\n",
        "            patience (int): val_lossが最後に改善されてからpatienceで指定された回まで改善なければ終了\n",
        "                            Default: 7\n",
        "            verbose (bool): Trueなら, val_lossの改善状況を示す\n",
        "                            Default: False\n",
        "            delta (float):  改善と認定するための最小変化量. delta以下の改善は改善と認めない\n",
        "                            Default: 0\n",
        "            path (str): PyTorchモデルを保存するパス\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): val_lossが更新されないとき，それが何回目を示す\n",
        "                            Default: print\n",
        "        '''\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        \n",
        "        # -val_lossをscoreにすることで，val_lossを最小化する目標から，scoreを最大化する目標へ\n",
        "        score = -val_loss\n",
        "\n",
        "        # best_scoreがなければscoreを当面のbest_scoreにする\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        # scoreがbest_score+deltaに満たなければ棄却\n",
        "        elif score < self.best_score + self.delta:\n",
        "            # 棄却カウント1追加\n",
        "            self.counter += 1\n",
        "            self.trace_func(\n",
        "                f'EarlyStopping counter: {self.counter} out of {self.patience}'\n",
        "            )\n",
        "            # 棄却カウントの合計がpatienceを超えたとき\n",
        "            if self.counter >= self.patience:\n",
        "                # Early stopで中止\n",
        "                self.early_stop = True\n",
        "        # scoreがそれまでのbest_scoreより大きいとき\n",
        "        else:\n",
        "            # scoreを更新\n",
        "            self.best_score = score\n",
        "            # modelとval_lossを保存\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            # 棄却カウントを0に戻す\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''val_lossが減少した際のmodelを保存する関数'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(\n",
        "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...'\n",
        "            )\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LZePJfhghTP"
      },
      "source": [
        "##### 入力 5-37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIzHHro-ghTQ"
      },
      "outputs": [],
      "source": [
        "# エポック数は400にする\n",
        "EPOCHS = 400\n",
        "\n",
        "# early_stoppingを定義\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "# モデルとオプティマイザを最初の条件に戻す\n",
        "model = BinaryClassificationNet()\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "model.train()\n",
        "# トレーニングと検証を同時に行うforループを作る\n",
        "# この節ではまだテストデータセットは使わない\n",
        "\n",
        "# 損失関数の出力とaccuracyはリストにまとめて後で図示する\n",
        "train_loss_li = []\n",
        "train_acc_li = []\n",
        "val_loss_li = []\n",
        "val_acc_li = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    for X_batch_train, y_batch_train in train_loader:\n",
        "        X_batch_train, y_batch_train = X_batch_train.to(device), y_batch_train.to(\n",
        "            device\n",
        "        )\n",
        "        \n",
        "        y_logits = model(X_batch_train).squeeze()\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "        # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する. ここではaccも使う\n",
        "        loss = loss_fn(y_logits, y_batch_train)\n",
        "        acc = binary_acc(y_true=y_batch_train, y_pred=y_pred)\n",
        "\n",
        "        # 3. オプティマイザの勾配を0に初期化し，過去の勾配の影響を排除して現在の勾配を計算できるようにする\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. 誤差逆伝播法で損失関数の勾配を計算する\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. オプティマイザで重みなどのパラメータ更新を行う\n",
        "        optimizer.step()\n",
        "\n",
        "    # 訓練データセットでミニバッチ学習が終わったところで，検証データセットの結果も確認する\n",
        "    model.eval()\n",
        "\n",
        "    # 検証では勾配が変動してはならないため，torch.no_grad()でテンソルの勾配計算ができないようにする\n",
        "    with torch.no_grad():\n",
        "        for X_batch_val, y_batch_val in val_loader:\n",
        "            X_batch_val, y_batch_val = X_batch_val.to(device), y_batch_val.to(device)\n",
        "            \n",
        "            # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "            val_logits = model(X_batch_val).squeeze()\n",
        "            val_pred = torch.round(torch.sigmoid(val_logits))\n",
        "            # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する.ここではaccも使う\n",
        "            val_loss = loss_fn(val_logits, y_batch_val)\n",
        "            val_acc = binary_acc(y_true=y_batch_val, y_pred=val_pred)\n",
        "            # 3以降は勾配の計算に関わってくるため，検証やテストでは使わない\n",
        "    \n",
        "    # 10エポックごとの結果をまとめる\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch} | Train_loss: {loss:.5f}, Train_acc: {acc:.2f}%')\n",
        "        print(f'Epoch: {epoch} | Val_loss: {val_loss:.5f}, Val_acc: {val_acc:.2f}%')\n",
        "    # 最後に，train_loss，train_acc，val_loss，val_accをリストにまとめる\n",
        "    train_loss_li.append(loss.item())\n",
        "    train_acc_li.append(acc)\n",
        "    val_loss_li.append(val_loss.item())\n",
        "    val_acc_li.append(val_acc)\n",
        "\n",
        "    # early_stoppingを適用\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        # 一定epochだけval_lossが最低値を更新しなかった場合，ここに入り学習を終了\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sg7SZDVDzIc"
      },
      "source": [
        "##### 入力5-38"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtB6xqhbghTQ"
      },
      "outputs": [],
      "source": [
        "# 全400エポックで設定したところで途中でearly stopした結果\n",
        "\n",
        "# 結局何エポックだったかを確認\n",
        "print(len(val_loss_li))\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "# add_subplot(121)とは，1*2に図を並べる際の1番目を指定している\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.set_xlabel('epochs', size=16)\n",
        "ax1.set_ylabel('loss', size=16)\n",
        "ax1.plot(train_loss_li, label='train_loss')\n",
        "ax1.plot(val_loss_li, color='#ff7f0e', label='val_loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.set_ylabel('acc', size=16)\n",
        "ax2.set_xlabel('epochs', size=16)\n",
        "ax2.plot(train_acc_li, label='train_acc')\n",
        "ax2.plot(val_acc_li, color='#ff7f0e', label='val_acc')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbj_9oZ3ghTQ"
      },
      "source": [
        "##### 入力5-39"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu8bdYIJghTR"
      },
      "outputs": [],
      "source": [
        "# itertoolsでリストを平坦化する\n",
        "import itertools\n",
        "\n",
        "# early_stoppingした時のモデルは保存されている. これをロードする\n",
        "model_path = 'checkpoint.pt'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# 予測確率をリストにまとめる\n",
        "test_prob_list = []\n",
        "# 予測ラベルをリストにまとめる\n",
        "test_pred_list = []\n",
        "\n",
        "# テストデータセットの結果を確認する\n",
        "model.eval()\n",
        "# テストでは勾配が変動してはならないため，torch.no_grad()でテンソルの勾配計算ができないようにする.\n",
        "with torch.no_grad():\n",
        "    for X_batch_test in test_loader:\n",
        "        X_batch_test = X_batch_test.to(device)\n",
        "        test_logits = model(X_batch_test).squeeze()\n",
        "        # 確率をリストに\n",
        "        test_prob = torch.sigmoid(test_logits)\n",
        "        test_prob_list.append(test_prob.cpu().numpy())\n",
        "        # ラベルをリストに\n",
        "        test_pred = torch.round(test_prob)\n",
        "        test_pred_list.append(test_pred.cpu().numpy())\n",
        "\n",
        "test_prob_list = list(itertools.chain.from_iterable(test_prob_list))\n",
        "test_pred_list = list(itertools.chain.from_iterable(test_pred_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPVLXtysST1F"
      },
      "source": [
        "##### 入力5-40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhsUxtaO_BdN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# ROC-AUC\n",
        "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_prob_list)\n",
        "auc_test = auc(fpr_test, tpr_test)\n",
        "\n",
        "# ROC曲線を描く\n",
        "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
        "\n",
        "plt.plot(\n",
        "      fpr_test, tpr_test, label='EarlyStopping+NeuralNetwork (AUC = %.3f)' % auc_test\n",
        ")\n",
        "plt.fill_between(fpr_test, tpr_test, 0, alpha=0.1)\n",
        "\n",
        "plt.legend()\n",
        "plt.title('EarlyStopping+NeuralNetwork')\n",
        "plt.xlabel('False_Positive_Rate')\n",
        "plt.ylabel('True_Positive_Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjI2X14ZEV-8"
      },
      "source": [
        "##### 入力5-41"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeE7ZbqTqQ-O"
      },
      "outputs": [],
      "source": [
        "# classification_report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, test_pred_list))\n",
        "print(classification_report(y_test, test_pred_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pRMEP51EUm5"
      },
      "source": [
        "##### 入力5-42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI34Cu3jghTT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BinaryClassificationNet(nn.Module):\n",
        "    # __init__()関数で使用するレイヤーを定義\n",
        "    def __init__(self, trial, num_layers, num_features):\n",
        "        super(BinaryClassificationNet, self).__init__()\n",
        "\n",
        "        # for loopの作りやすさから，nn.Sequentialではなくnn.ModuleListに\n",
        "        # appendしていく形をとる\n",
        "        # 第1層\n",
        "        # 入力特徴量は13，出力はnum_features[0]にしておいた\n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.Linear(in_features=13, out_features=num_features[0])]\n",
        "        )\n",
        "        self.layers.append(nn.BatchNorm1d(num_features[0]))\n",
        "        self.layers.append(nn.ReLU())\n",
        "\n",
        "        # 第2層以降\n",
        "        # 先の出力はnum_features[0]だった.for文で層を重ねていく\n",
        "        for i in range(1, num_layers):\n",
        "            self.layers.append(\n",
        "                nn.Linear(in_features=num_features[i - 1], out_features=num_features[i])\n",
        "            )\n",
        "            self.layers.append(nn.BatchNorm1d(num_features[i]))\n",
        "            self.layers.append(nn.ReLU())\n",
        "        # 最後に先の出力のnum_features[i]を1にする. これは0である確率1つだけということを言う\n",
        "        self.layers_out = nn.Linear(in_features=num_features[i], out_features=1)\n",
        "\n",
        "    # 関数forward()では定義済みレイヤーを呼び出す\n",
        "    def forward(self, x):\n",
        "        for i, l in enumerate(self.layers):\n",
        "            # l(x)でモデルを使う\n",
        "            x = l(x)\n",
        "        # sigmoidは損失関数に含まれているので設定しなくてok\n",
        "        x = self.layers_out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlvIDsNZ_BdO"
      },
      "source": [
        "##### 入力5-43"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE_d2y0lghTT"
      },
      "outputs": [],
      "source": [
        "# どのoptimizerを使うかというところからも最適化する\n",
        "import torch.optim as optim\n",
        "\n",
        "def get_optimizer(trial, model):\n",
        "    optimizer_names = ['Adam', 'RMSprop']\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', optimizer_names)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
        "    if optimizer_name == optimizer_names[0]:\n",
        "        Adam_lr = trial.suggest_float('Adam_lr', 1e-5, 1e-1, log=True)\n",
        "        optimizer = optim.Adam(\n",
        "            model.parameters(), lr=Adam_lr, weight_decay=weight_decay\n",
        "        )\n",
        "    else:\n",
        "        RMSprop_lr = trial.suggest_float('RMSprop_lr', 1e-5, 1e-1, log=True)\n",
        "        optimizer = optim.RMSprop(\n",
        "            model.parameters(), lr=RMSprop_lr, weight_decay=weight_decay\n",
        "        )\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRTfcnJ3TSTF"
      },
      "source": [
        "##### 入力5-44"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVfKBrnCqQ-R"
      },
      "outputs": [],
      "source": [
        "# 損失関数も定義しておく\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def train(model, device, train_loader, optimizer):\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        for X_batch_train, y_batch_train in train_loader:\n",
        "            X_batch_train, y_batch_train = X_batch_train.to(device), y_batch_train.to(\n",
        "                device\n",
        "            )\n",
        "\n",
        "            # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "            y_logits = model(X_batch_train).squeeze()\n",
        "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "            \n",
        "            # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する\n",
        "            loss = loss_fn(y_logits, y_batch_train)\n",
        "\n",
        "            # 3. オプティマイザの勾配を0に初期化し，過去の勾配の影響を排除して現在の勾配を計算できるようにする\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 4. 誤差逆伝播法で損失関数の勾配を計算する\n",
        "            loss.backward()\n",
        "\n",
        "            # 5. オプティマイザで重みなどのパラメータ更新を行う\n",
        "            optimizer.step()\n",
        "\n",
        "def val(model, device, val_loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_batch_val, y_batch_val in val_loader:\n",
        "            X_batch_val, y_batch_val = X_batch_val.to(device), y_batch_val.to(device)\n",
        "\n",
        "            # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "            val_logits = model(X_batch_val).squeeze()\n",
        "            val_pred = torch.round(torch.sigmoid(val_logits))\n",
        "            # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する\n",
        "            val_loss = loss_fn(val_logits, y_batch_val)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeMTv7aM_BdP"
      },
      "source": [
        "##### 入力5-45"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leKizH1wqQ-R"
      },
      "outputs": [],
      "source": [
        "# エポック数は増やしすぎると計算量が上がる\n",
        "# 5.1.3項の例で，30程度あればニューラルネットワークには十分に考えられたため30にしておくが，\n",
        "# 時間がかかりすぎる場合は20などに減らすこともできる\n",
        "EPOCHS = 30\n",
        "\n",
        "def objective_nn(trial):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 層の数は3から7に\n",
        "    num_layers = trial.suggest_int('num_layer', 3, 7)\n",
        "    # 全結合層の特徴量数\n",
        "    num_features = [\n",
        "        int(trial.suggest_float(f'num_features_{i}', 16, 128, step=16))\n",
        "        for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    model = BinaryClassificationNet(trial, num_layers, num_features).to(device)\n",
        "    optimizer = get_optimizer(trial, model)\n",
        "\n",
        "    for step in range(EPOCHS):\n",
        "        train(model, device, train_loader, optimizer)\n",
        "        val_loss = val(model, device, val_loader)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8gR6Qoe_BdP"
      },
      "source": [
        "##### 入力5-46"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nBL-JVE_BdQ"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "import optuna\n",
        "\n",
        "obj_nn = partial(objective_nn)\n",
        "# セッションの作成\n",
        "sampler = optuna.samplers.TPESampler(seed=0)\n",
        "\n",
        "# val_lossを最小化するためにdirection='minimize'にする\n",
        "study_nn = optuna.create_study(sampler=sampler, direction='minimize')\n",
        "# 回数を指定する.50回だとおよそ10分程度かかる\n",
        "study_nn.optimize(obj_nn, n_trials=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzM2ajEsF7x0"
      },
      "source": [
        "##### 入力5-47"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdWAslDD_BdQ"
      },
      "outputs": [],
      "source": [
        "study_nn.best_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2SAwNGEGAIz"
      },
      "source": [
        "##### 入力5-48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_6-8MFbST1I"
      },
      "outputs": [],
      "source": [
        "nn_params = study_nn.best_params\n",
        "nn_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys8gM0d6_BdR"
      },
      "source": [
        "##### 入力5-49"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qYXpJq9ghTU"
      },
      "outputs": [],
      "source": [
        "class BinaryClassificationNet(nn.Module):\n",
        "    # __init__()関数で使用するレイヤーを定義\n",
        "    def __init__(self):\n",
        "    \n",
        "        # 基底クラスとしてnn.moduleを継承する\n",
        "        # ニューラルネットワークの基本機能が使えるようになるおまじないと考えてもよい\n",
        "        super(BinaryClassificationNet, self).__init__()\n",
        "\n",
        "        # for loopの作りやすさから，nn.Sequentialではなくnn.ModuleListに\n",
        "        # appendしていく形をとる\n",
        "        # 第1層\n",
        "        # 入力特徴量は13，出力はnum_features[0]にしておいた\n",
        "        self.layers = nn.ModuleList(\n",
        "           [nn.Linear(in_features=13, out_features=num_features[0])]\n",
        "        )\n",
        "        self.layers.append(nn.BatchNorm1d(num_features[0]))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        # 第2層以降\n",
        "        # 先の出力はnum_features[0]だった.for文で層を重ねていく\n",
        "        for i in range(1, num_layers):\n",
        "           self.layers.append(\n",
        "              nn.Linear(in_features=num_features[i - 1], out_features=num_features[i])\n",
        "            )\n",
        "           self.layers.append(nn.BatchNorm1d(num_features[i]))\n",
        "           self.layers.append(nn.ReLU())\n",
        "\n",
        "        # 最後に先の出力のnum_features[i]を1にする. これは0である確率1つだけということを言う\n",
        "        self.layers_out = nn.Linear(in_features=num_features[i], out_features=1)\n",
        "    \n",
        "    # 関数forward()では定義済みレイヤーを呼び出す\n",
        "    def forward(self, x):\n",
        "        for i, l in enumerate(self.layers):\n",
        "            # l(x)でモデルを使う\n",
        "            x = l(x)\n",
        "        # sigmoidは損失関数に含まれているので設定しなくてok\n",
        "        x = self.layers_out(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_xFk6wOST1I"
      },
      "source": [
        "##### 入力5-50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyaa0kgZghTU"
      },
      "outputs": [],
      "source": [
        "# 最適化したハイパーパラメータ情報を入れる\n",
        "num_layers = nn_params['num_layer']\n",
        "# リスト内包表記で各パラメータを用意\n",
        "# num_featuresはint()で32.0→32としないとエラーを起こす\n",
        "num_features = [\n",
        "      int(nn_params[f'num_features_{i}']) for i in range(nn_params['num_layer'])\n",
        "]\n",
        "\n",
        "# モデルを構築\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = BinaryClassificationNet()\n",
        "# モデルをGPUへ\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 最適化したoptimizerを使用する\n",
        "optimizer_names = ['Adam', 'RMSprop']\n",
        "weight_decay = nn_params['weight_decay']\n",
        "if nn_params['optimizer'] == optimizer_names[0]:\n",
        "    Adam_lr = nn_params['Adam_lr']\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Adam_lr, weight_decay=weight_decay)\n",
        "else:\n",
        "    RMSprop_lr = nn_params['RMSprop_lr']\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=RMSprop_lr, weight_decay=weight_decay)\n",
        "\n",
        "# 学習過程でうまく学習できているか監視するために，損失関数だけでなく\n",
        "# accuracyも見ることにする. そのためにもaccuracyを確認する関数を定義する\n",
        "\n",
        "def binary_acc(y_true, y_pred):\n",
        "    # torch.eq()で2つのテンソルで値が一致する数をカウントする\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4oUYEVEHBvk"
      },
      "source": [
        "##### 入力5-51"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5mpFWlaghTV"
      },
      "outputs": [],
      "source": [
        "# エポック数は30にする\n",
        "EPOCHS = 30\n",
        "model.train()\n",
        "# トレーニングと検証を同時に行うforループを作る\n",
        "# この節ではまだテストデータセットは使わない\n",
        "\n",
        "# 損失関数の出力とaccuracyはリストにまとめて後で図示する\n",
        "train_loss_li = []\n",
        "train_acc_li = []\n",
        "val_loss_li = []\n",
        "val_acc_li = []\n",
        "\n",
        "# エポック: すべての入力データを何回使えたかを示す\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # イテレーション: 重みを何度更新したかを示す. バッチサイズ64で，\n",
        "\n",
        "    # 訓練では，まず，ミニバッチごとに分けられたデータを抽出する\n",
        "    for X_batch_train, y_batch_train in train_loader:\n",
        "\n",
        "        # ミニバッチのデータをGPUに適用する\n",
        "        X_batch_train, y_batch_train = X_batch_train.to(device), y_batch_train.to(\n",
        "            device\n",
        "        )\n",
        "\n",
        "        # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "        y_logits = model(X_batch_train).squeeze()\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "        # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する.ここではaccも使う\n",
        "        loss = loss_fn(y_logits, y_batch_train)\n",
        "        acc = binary_acc(y_true=y_batch_train, y_pred=y_pred)\n",
        "\n",
        "        # 3. オプティマイザの勾配を0に初期化し，過去の勾配の影響を排除して現在の勾配を計算でき るようにする\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. 誤差逆伝播法で損失関数の勾配を計算する\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. オプティマイザで重みなどのパラメータ更新を行う\n",
        "        optimizer.step()\n",
        "\n",
        "    # 検証データでの確認\n",
        "    # 訓練データの後に検証データを確認している\n",
        "    model.eval()\n",
        "    # torch.no_grad()でテンソルの勾配計算ができないようにする\n",
        "    with torch.no_grad():\n",
        "        for X_batch_val, y_batch_val in val_loader:\n",
        "            X_batch_val, y_batch_val = X_batch_val.to(device), y_batch_val.to(device)\n",
        "\n",
        "            # 1. Forward関数を使って，図で言えば左から右の順伝播処理を作る\n",
        "            val_logits = model(X_batch_val).squeeze()\n",
        "            val_pred = torch.round(torch.sigmoid(val_logits))\n",
        "            # 2. 損失関数を使って，どの程度モデルが誤っているかを確認する.ここではaccも使う\n",
        "            val_loss = loss_fn(val_logits, y_batch_val)\n",
        "            val_acc = binary_acc(y_true=y_batch_val, y_pred=val_pred)\n",
        "            # 3以降は勾配の計算に関わってくるため，検証やテストでは書かない\n",
        "        if epoch % 2 == 0:\n",
        "            print(\n",
        "                f'Epoch: {epoch} | Train_loss: {loss:.5f}, Train_acc: {acc:.2f}% | Val_loss: {val_loss:.5f}, Val_acc: {val_acc:.2f}%'\n",
        "            )\n",
        "    val_loss_li.append(val_loss.item())\n",
        "    val_acc_li.append(val_acc)\n",
        "\n",
        "    train_loss_li.append(loss.item())\n",
        "    train_acc_li.append(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPE_MKIV6wk"
      },
      "source": [
        "##### 入力5-52"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL0_CkIUghTZ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.set_ylabel('loss', size=16)\n",
        "ax1.set_ylabel('loss', size=16)\n",
        "ax1.plot(train_loss_li, label='train_loss')\n",
        "ax1.plot(val_loss_li, color='#ff7f0e', label='val_loss')\n",
        "ax1.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.set_ylabel('acc', size=16)\n",
        "ax2.plot(train_acc_li, label='train_acc')\n",
        "ax2.plot(val_acc_li, color='#ff7f0e', label='val_acc')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EstBqdZQST1M"
      },
      "source": [
        "##### 入力5-53"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrpP8ZW8ghTZ"
      },
      "outputs": [],
      "source": [
        "# itertoolsでリストを平坦化する\n",
        "import itertools\n",
        "\n",
        "# 予測確率をリストにまとめる\n",
        "test_prob_list = []\n",
        "# 予測ラベルをリストにまとめる\n",
        "test_pred_list = []\n",
        "\n",
        "# テストデータセットの結果を確認する\n",
        "model.eval()\n",
        "# テストでは勾配が変動してはならないため，torch.no_grad()でテンソルの勾配計算ができないように する.\n",
        "with torch.no_grad():\n",
        "      for X_batch_test in test_loader:\n",
        "          X_batch_test = X_batch_test.to(device)\n",
        "          test_logits = model(X_batch_test).squeeze()\n",
        "          # 確率をリストに\n",
        "          test_prob = torch.sigmoid(test_logits)\n",
        "          test_prob_list.append(test_prob.cpu().numpy())\n",
        "          # ラベルをリストに\n",
        "          test_pred = torch.round(test_prob)\n",
        "          test_pred_list.append(test_pred.cpu().numpy())\n",
        "\n",
        "test_prob_list = list(itertools.chain.from_iterable(test_prob_list))\n",
        "test_pred_list = list(itertools.chain.from_iterable(test_pred_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH4UsHWOST1M"
      },
      "source": [
        "##### 入力5-54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKGQ6KQJghTa"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import auc, roc_curve\n",
        "\n",
        "# ROC-AUC\n",
        "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, test_prob_list)\n",
        "auc_test = auc(fpr_test, tpr_test)\n",
        "\n",
        "# ROC曲線を描く\n",
        "plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
        "\n",
        "plt.plot(fpr_test, tpr_test, label='Optuna+NeuralNetwork (AUC = %.3f)' % auc_test)\n",
        "plt.fill_between(fpr_test, tpr_test, 0, alpha=0.1)\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Optuna+NeuralNetwork')\n",
        "plt.xlabel('False_Positive_Rate')\n",
        "plt.ylabel('True_Positive_Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYrQh9ZkWkao"
      },
      "source": [
        "##### 入力5-55"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31O8UaRTghTa"
      },
      "outputs": [],
      "source": [
        "# classification_report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, test_pred_list))\n",
        "print(classification_report(y_test, test_pred_list))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tgxfGbruOFjX"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
